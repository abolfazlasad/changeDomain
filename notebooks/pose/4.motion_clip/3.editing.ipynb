{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb55036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/asad/workspace/DomainProject/changeDomain/notebooks/pose/4.motion_clip/MotionCLIP\n"
     ]
    }
   ],
   "source": [
    "if \"_magic_done\" not in globals(): # prevent multiple run\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    %cd ./MotionCLIP\n",
    "    _magic_done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97acf633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# MotionClip\n",
    "from src.parser.visualize import parser\n",
    "from src.utils.misc import load_model_wo_clip\n",
    "from src.datasets.get_dataset import get_datasets\n",
    "from src.models.get_model import get_model as get_gen_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bea0ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\n",
    "    \"notebook\",  # dummy script name\n",
    "    \"./exps/paper-model/checkpoint_0100.pth.tar\",\n",
    "    \"--input_file\", \"./assets/paper_edits.csv\",\n",
    "]\n",
    "\n",
    "\n",
    "parameters, folder, checkpointname, epoch = parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8cc944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=parameters['device'], jit=False)  # Must set jit=False for training\n",
    "clip.model.convert_weights(clip_model)  # Actually this line is unnecessary since clip by default already on float16\n",
    "\n",
    "if parameters.get('clip_training', '') == '':\n",
    "    clip_model.eval()\n",
    "    for p in clip_model.parameters():\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2288f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapath used by amass is [./data/amass_db/amass_30fps_test.pt]\n"
     ]
    }
   ],
   "source": [
    "split='test'\n",
    "# split='all'  # need more memory\n",
    "datasets = get_datasets(parameters, clip_preprocess, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4cdb7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asad/workspace/anaconda3/envs/motionclip_py310_v2/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2030523/545624579.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(checkpointpath, map_location=parameters[\"device\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore weights..\n"
     ]
    }
   ],
   "source": [
    "# from src.config import SMPL_DATA_PATH, SMPL_MODEL_PATH\n",
    "# SMPL_DATA_PATH = \"ok\"\n",
    "# SMPL_MODEL_PATH = os.path.join(SMPL_DATA_PATH, \"SMPL_NEUTRAL.pkl\")\n",
    "model = get_gen_model(parameters, clip_model)\n",
    "\n",
    "print(\"Restore weights..\")\n",
    "checkpointpath = os.path.join(folder, checkpointname)\n",
    "state_dict = torch.load(checkpointpath, map_location=parameters[\"device\"])\n",
    "load_model_wo_clip(model, state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "907bc1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def retrieve_motions(datasets, motion_collection, texts, device):\n",
    "#     retrieved_motions = []\n",
    "#     for txt in texts:\n",
    "#         _split, _index = motion_collection[txt][0]\n",
    "#         retrieved_motions.append(datasets[_split][_index]['inp'].unsqueeze(0).to(device))\n",
    "#     return torch.cat(retrieved_motions, axis=0)\n",
    "\n",
    "def retrieve_motions(inp_list, device):\n",
    "    retrieved_motions = []\n",
    "    for inp in inp_list:\n",
    "        retrieved_motions.append(inp.unsqueeze(0).to(device))\n",
    "    return torch.cat(retrieved_motions, axis=0)\n",
    "\n",
    "\n",
    "def encode_motions(model, motions, device):\n",
    "    return model.encoder({'x': motions,\n",
    "                          'y': torch.zeros(motions.shape[0], dtype=int, device=device),\n",
    "                          'mask': model.lengths_to_mask(torch.ones(motions.shape[0], dtype=int, device=device) * 60)})[\"mu\"]\n",
    "\n",
    "inp_list = [\n",
    "    datasets[\"test\"][0][\"inp\"],\n",
    "    datasets[\"test\"][2][\"inp\"],\n",
    "    datasets[\"test\"][3][\"inp\"],\n",
    "    datasets[\"test\"][-1][\"inp\"],\n",
    "]\n",
    "retrieved_motions = retrieve_motions(inp_list, parameters['device'])\n",
    "clip_features1 = encode_motions(model, retrieved_motions[:, :, :, :], parameters['device'])\n",
    "clip_features1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4307136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75191e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or use clip\n",
    "clip_features2 = []\n",
    "for text in [\n",
    "    \"jump\",\n",
    "    \"old walk\",\n",
    "    \"drunk walk\",\n",
    "]:\n",
    "    clip_tokens = clip.tokenize(text).to(parameters['device'])\n",
    "    clip_features2.append(model.clip_model.encode_text(clip_tokens).float())\n",
    "clip_features2.append(clip_features2[0] + clip_features2[1] - clip_features2[2])\n",
    "clip_features2 = torch.cat(clip_features2)\n",
    "clip_features2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87d40f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e39f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clip_features = [\n",
    "    clip_features1,\n",
    "    clip_features2\n",
    "]\n",
    "\n",
    "\n",
    "all_clip_features = torch.transpose(torch.stack(all_clip_features, axis=0), 0, 1)\n",
    "h, w = all_clip_features.shape[:2]\n",
    "gendurations = torch.ones((h*w, 1), dtype=int) * parameters['num_frames']\n",
    "\n",
    "# generate the repr (joints3D/pose etc)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generation = model.generate(all_clip_features, gendurations,\n",
    "                                is_amass=True,\n",
    "                                is_clip_features=True)\n",
    "\n",
    "for key, val in generation.items():\n",
    "    if len(generation[key].shape) == 1:\n",
    "        generation[key] = val.reshape(h, w)\n",
    "    else:\n",
    "        generation[key] = val.reshape(h, w, *val.shape[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57bfa820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 25, 6, 60])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation[\"output\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0cbda09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils.rotation_conversions as geometry\n",
    "\n",
    "def inp_to_theta(inp):\n",
    "    ret = inp.permute(2, 0, 1)\n",
    "\n",
    "    # Separate rotations and translation\n",
    "    rot_part = ret[:, :24, :]  # (nframes, 24, 6)\n",
    "\n",
    "    # Revert rot6d to matrix to axis_angle\n",
    "    matrix = geometry.rotation_6d_to_matrix(rot_part)\n",
    "    axis_angle = geometry.matrix_to_axis_angle(matrix)  # (nframes, 24, 3)\n",
    "\n",
    "    # Flatten to thetas\n",
    "    return axis_angle.reshape(axis_angle.shape[0], -1)  # (nframes, 72)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03169d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from smplx import SMPL\n",
    "import trimesh\n",
    "import pyrender\n",
    "import numpy as np\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def thetas_to_video(motion, output_path):\n",
    "    # Load SMPL model\n",
    "    smpl_model = SMPL(model_path='./models/smpl')\n",
    "    faces = smpl_model.faces\n",
    "\n",
    "    scene = pyrender.Scene()\n",
    "    r = pyrender.OffscreenRenderer(640, 480)\n",
    "\n",
    "    # Add a camera\n",
    "    camera = pyrender.PerspectiveCamera(yfov=np.pi / 3.0)\n",
    "    cam_pose = np.array(\n",
    "        # [\n",
    "        #     [ 1.00, 0.05,-0.03,-0.10],\n",
    "        #     [-0.05, 0.25,-0.97,-2.78],\n",
    "        #     [-0.04, 0.97, 0.25, 0.61],\n",
    "        #     [ 0.  , 0.  , 0.  , 1.  ]\n",
    "        # ]\n",
    "        # find camera by code from notebook 2.smpl_dataVisualization.ipynb\n",
    "        [\n",
    "            [ 9.98793959e-01,  1.16659486e-03,  4.90842806e-02, 1.18703522e-01],\n",
    "            [-7.82186846e-04, -9.99212735e-01,  3.96648358e-02, -1.85050091e-01],\n",
    "            [ 4.90919110e-02, -3.96553915e-02, -9.98006731e-01, -2.73471684e+00],\n",
    "            [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, 1.00000000e+00]]\n",
    "    )\n",
    "    scene.add(camera, pose=cam_pose)\n",
    "\n",
    "    # Add light\n",
    "    light = pyrender.DirectionalLight(color=np.ones(3), intensity=3.0)\n",
    "    scene.add(light, pose=cam_pose)\n",
    "\n",
    "    # Suppose data[\"thetas\"][0] has shape [T, 72]\n",
    "    frames = []\n",
    "    T = len(motion)\n",
    "    if isinstance(motion, np.ndarray):\n",
    "        motion = torch.from_numpy(motion)\n",
    "    motion = motion.to(torch.float)\n",
    "    for i in tqdm(range(T), desc=\"Rendering frames\"):\n",
    "        theta = motion[i:i+1]\n",
    "        output = smpl_model(body_pose=theta[:, 3:], global_orient=theta[:, :3])\n",
    "\n",
    "        vertices = output.vertices.detach().cpu().numpy().squeeze()\n",
    "        mesh_visual = pyrender.Mesh.from_trimesh(trimesh.Trimesh(vertices, faces))\n",
    "\n",
    "        # Clear old mesh, add new one\n",
    "        scene.clear()\n",
    "        scene.add(camera, pose=cam_pose)\n",
    "        scene.add(light, pose=cam_pose)\n",
    "        scene.add(mesh_visual)\n",
    "\n",
    "        # Render and save frame\n",
    "        color, _ = r.render(scene)\n",
    "        frames.append(color)\n",
    "\n",
    "    r.delete()\n",
    "\n",
    "\n",
    "    # Make sure ffmpeg is installed: pip install imageio[ffmpeg]\n",
    "    with imageio.get_writer(output_path, fps=30, format='ffmpeg') as writer:\n",
    "        for frame in frames:\n",
    "            writer.append_data(frame)\n",
    "\n",
    "    print(f\"✅ Saved video as {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f61a921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering frames: 100%|██████████| 60/60 [00:00<00:00, 105.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved video as ../output/3.encode_decode_with_motionClip.mp4\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering frames: 100%|██████████| 60/60 [00:00<00:00, 107.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved video as ../output/3.edit_with_motionClip.mp4\n"
     ]
    }
   ],
   "source": [
    "theta = inp_to_theta(generation[\"output\"][3,0]).cpu()\n",
    "thetas_to_video(theta, \"../output/3.encode_decode_with_motionClip.mp4\")\n",
    "\n",
    "theta = inp_to_theta(generation[\"output\"][3,1]).cpu()\n",
    "thetas_to_video(theta, \"../output/3.edit_with_motionClip.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motionclip_py310_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
